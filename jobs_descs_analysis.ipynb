{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "jobs = json.load(open(\"/Users/kalyani/01ColumbiaQMSS/01Semester2/NLP/Homework_2/indeed_title_job_desc(1).json\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer, ToktokTokenizer, TweetTokenizer\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenizers = {'tree': TreebankWordTokenizer(),\n",
    "              'toktok': ToktokTokenizer()}\n",
    "\n",
    "stemmers = {'porter': PorterStemmer(),\n",
    "            'stemmer': SnowballStemmer('english')}\n",
    "\n",
    "word_counts = {}\n",
    "word_counts_titles = {}\n",
    "\n",
    "def create_key(tokenizer, stemmer):\n",
    "    return str(tokenizer) + \":\" + str(stemmer)\n",
    "    \n",
    "for tokenizer, stemmer in product(tokenizers.keys(), stemmers.keys()):\n",
    "    word_counts[create_key(tokenizer, stemmer)] = []\n",
    "\n",
    "count = 0\n",
    "df = pd.DataFrame(columns = [\"word\", \"count\"])    \n",
    "# Loop over the functions and sentences\n",
    "for tokenizer, stemmer, job_title in product(tokenizers,\n",
    "                                            stemmers, jobs.keys()):\n",
    "    key_title = create_key(tokenizer, stemmer) + \", \" + job_title\n",
    "    word_counts_titles[key_title] = []\n",
    "\n",
    "    for desc in jobs[job_title]:\n",
    "        toks = tokenizers[tokenizer].tokenize(desc)\n",
    "        stems = [stemmers[stemmer].stem(tok) for tok in toks]\n",
    "        words = Counter(stems)\n",
    "        mat = CountVectorizer(words)\n",
    "        key = create_key(tokenizer, stemmer) \n",
    "        word_counts[key].append(dict(mat.input))\n",
    "        word_counts_titles[key_title].append(dict(mat.input))\n",
    "    df1 = pd.DataFrame(word_counts_titles[key_title])\n",
    "    key_title = key_title.split(', ')\n",
    "    title = key_title[1]\n",
    "    tokenizer_stemmer = key_title[0]\n",
    "    df1['title'] =  [str(title)] * len(df1)\n",
    "    df1['tokenizer_stemmer'] = [str(tokenizer_stemmer)] * len(df1)\n",
    "    df = pd.concat([df, df1], sort=True)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>#1</th>\n",
       "      <th>#datavisu</th>\n",
       "      <th>#ind123</th>\n",
       "      <th>#li</th>\n",
       "      <th>#li-bf</th>\n",
       "      <th>#li-bj1</th>\n",
       "      <th>#li-ct1</th>\n",
       "      <th>...</th>\n",
       "      <th>“up</th>\n",
       "      <th>“valu</th>\n",
       "      <th>“voic</th>\n",
       "      <th>“volans-i</th>\n",
       "      <th>“what-if</th>\n",
       "      <th>“x</th>\n",
       "      <th>”</th>\n",
       "      <th>•</th>\n",
       "      <th>…</th>\n",
       "      <th>﻿we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11751 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    !   \"    #  #1  #datavisu  #ind123  #li  #li-bf  #li-bj1  #li-ct1 ...   \\\n",
       "0 NaN NaN  NaN NaN        NaN      NaN  NaN     NaN      NaN      NaN ...    \n",
       "1 NaN NaN  3.0 NaN        NaN      NaN  NaN     NaN      NaN      NaN ...    \n",
       "2 NaN NaN  NaN NaN        NaN      NaN  NaN     NaN      NaN      NaN ...    \n",
       "3 NaN NaN  1.0 NaN        NaN      NaN  NaN     NaN      NaN      NaN ...    \n",
       "4 NaN NaN  NaN NaN        NaN      NaN  NaN     NaN      NaN      NaN ...    \n",
       "\n",
       "   “up  “valu  “voic  “volans-i  “what-if  “x   ”   •   …  ﻿we  \n",
       "0  NaN    NaN    NaN        NaN       NaN NaN NaN NaN NaN  NaN  \n",
       "1  NaN    NaN    NaN        NaN       NaN NaN NaN NaN NaN  NaN  \n",
       "2  NaN    NaN    NaN        NaN       NaN NaN NaN NaN NaN  NaN  \n",
       "3  NaN    NaN    NaN        NaN       NaN NaN NaN NaN NaN  NaN  \n",
       "4  NaN    NaN    NaN        NaN       NaN NaN NaN NaN NaN  NaN  \n",
       "\n",
       "[5 rows x 11751 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree:porter\n",
      "735\n",
      "tree:stemmer\n",
      "735\n",
      "toktok:porter\n",
      "735\n",
      "toktok:stemmer\n",
      "735\n"
     ]
    }
   ],
   "source": [
    "for key in word_counts.keys():\n",
    "    print(key)\n",
    "    print(len(list(word_counts[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of the word count matrix for the four pre-processing specifications are: 735."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_n_numbers(list1, n):\n",
    "    return sorted(range(len(list1)), key=lambda i: list1[i])[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "logreg_dict = {}\n",
    "for tokenizer_stemmer in np.unique(df['tokenizer_stemmer']):\n",
    "    df1 = df.loc[df['tokenizer_stemmer'] == tokenizer_stemmer]\n",
    "    X = df1.loc[:, df1.columns != 'title']\n",
    "    X = X.loc[:, X.columns != 'tokenizer_stemmer']\n",
    "    X = X.fillna(0)\n",
    "    y = df1['title'].astype('category')\n",
    "    y1 = pd.DataFrame(y)\n",
    "    #y1 = pd.Series(LabelEncoder().fit_transform(y))\n",
    "    logreg = LogisticRegression(C=1e90).fit(X, y1)\n",
    "    coefs = list(logreg.coef_)\n",
    "    sums = []\n",
    "    index = 0\n",
    "    while index < len(coefs[0]):\n",
    "        sum = coefs[0][index]+ coefs[1][index] +coefs[2][index] +coefs[3][index] +coefs[4][index]\n",
    "        sums.append(sum)\n",
    "        index += 1\n",
    "    logreg_dict[tokenizer_stemmer] = highest_n_numbers(sums, 10)\n",
    "logreg = pd.DataFrame(logreg_dict)\n",
    "\n",
    "logreg_dict_words = {}\n",
    "\n",
    "meep = []\n",
    "\n",
    "for key in logreg_dict.keys():\n",
    "    logreg_dict_words[key] = []\n",
    "    for index in logreg_dict['toktok:porter']:\n",
    "        logreg_dict_words[key].append(df.columns[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the words with the top ten coefficients for each kind of pre-processing specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toktok:porter': ['divers',\n",
       "  'scalabl',\n",
       "  'We',\n",
       "  'an',\n",
       "  'better',\n",
       "  'workstreams.',\n",
       "  'employ',\n",
       "  'team.',\n",
       "  ';',\n",
       "  'solut'],\n",
       " 'toktok:stemmer': ['divers',\n",
       "  'scalabl',\n",
       "  'We',\n",
       "  'an',\n",
       "  'better',\n",
       "  'workstreams.',\n",
       "  'employ',\n",
       "  'team.',\n",
       "  ';',\n",
       "  'solut'],\n",
       " 'tree:porter': ['divers',\n",
       "  'scalabl',\n",
       "  'We',\n",
       "  'an',\n",
       "  'better',\n",
       "  'workstreams.',\n",
       "  'employ',\n",
       "  'team.',\n",
       "  ';',\n",
       "  'solut'],\n",
       " 'tree:stemmer': ['divers',\n",
       "  'scalabl',\n",
       "  'We',\n",
       "  'an',\n",
       "  'better',\n",
       "  'workstreams.',\n",
       "  'employ',\n",
       "  'team.',\n",
       "  ';',\n",
       "  'solut']}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     %    '    (    )     ,    -    .    2    3    5 ...   without  word  \\\n",
      "0  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "1  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "2  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "3  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "4  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "\n",
      "   work  written  www.coxcsrreport.com.  year  yes  yield  you    ’  \n",
      "0   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "1   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "2   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "3   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "4   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "\n",
      "[5 rows x 311 columns]\n",
      "     %    '    (    )     ,    -    .    2    3    5 ...   without  word  \\\n",
      "0  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "1  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "2  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "3  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "4  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "\n",
      "   work  written  www.coxcsrreport.com.  year  yes  yield  you    ’  \n",
      "0   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "1   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "2   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "3   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "4   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "\n",
      "[5 rows x 311 columns]\n",
      "     %    '    (    )     ,    -    .    2    3    5 ...   without  word  \\\n",
      "0  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "1  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "2  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "3  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "4  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "\n",
      "   work  written  www.coxcsrreport.com.  year  yes  yield  you    ’  \n",
      "0   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "1   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "2   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "3   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "4   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "\n",
      "[5 rows x 311 columns]\n",
      "     %    '    (    )     ,    -    .    2    3    5 ...   without  word  \\\n",
      "0  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "1  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "2  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "3  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "4  1.0  6.0  4.0  4.0  47.0  1.0  1.0  3.0  1.0  3.0 ...       1.0     0   \n",
      "\n",
      "   work  written  www.coxcsrreport.com.  year  yes  yield  you    ’  \n",
      "0   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "1   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "2   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "3   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "4   1.0      1.0                    1.0   6.0  1.0    1.0  1.0  1.0  \n",
      "\n",
      "[5 rows x 311 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer, ToktokTokenizer, TweetTokenizer\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizers = {'tree': TreebankWordTokenizer(),\n",
    "              'toktok': ToktokTokenizer()}\n",
    "\n",
    "stemmers = {'porter': PorterStemmer(),\n",
    "            'stemmer': SnowballStemmer('english')}\n",
    "\n",
    "word_counts = {}\n",
    "word_counts_titles = {}\n",
    "\n",
    "def create_key(tokenizer, stemmer):\n",
    "    return str(tokenizer) + \":\" + str(stemmer)\n",
    "    \n",
    "for tokenizer, stemmer in product(tokenizers.keys(), stemmers.keys()):\n",
    "    word_counts[create_key(tokenizer, stemmer)] = []\n",
    "\n",
    "count = 0\n",
    "df = pd.DataFrame(columns = [\"word\", \"count\"])    \n",
    "# Loop over the functions and sentences\n",
    "for tokenizer, stemmer, job_title in product(tokenizers,\n",
    "                                            stemmers, jobs.keys()):\n",
    "    key_title = create_key(tokenizer, stemmer) + \", \" + job_title\n",
    "    word_counts_titles[key_title] = []\n",
    "\n",
    "    for desc in jobs[job_title]:\n",
    "        toks = tokenizers[tokenizer].tokenize(desc)\n",
    "        stems = [stemmers[stemmer].stem(tok) for tok in toks]\n",
    "        words = Counter(stems)\n",
    "        tvectorizer = TfidfVectorizer(words, ngram_range=(1, 3))\n",
    "        key = create_key(tokenizer, stemmer) \n",
    "        word_counts[key].append(dict(mat.input))\n",
    "        word_counts_titles[key_title].append(dict(mat.input))\n",
    "    df1 = pd.DataFrame(word_counts_titles[key_title])\n",
    "    key_title = key_title.split(', ')\n",
    "    title = key_title[1]\n",
    "    tokenizer_stemmer = key_title[0]\n",
    "    df1['title'] =  [str(title)] * len(df1)\n",
    "    df1['tokenizer_stemmer'] = [str(tokenizer_stemmer)] * len(df1)\n",
    "    df = pd.concat([df, df1])\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "logreg_dict = {}\n",
    "for tokenizer_stemmer in np.unique(df['tokenizer_stemmer']):\n",
    "    df1 = df.loc[df['tokenizer_stemmer'] == tokenizer_stemmer]\n",
    "    X = df1.loc[:, df1.columns != 'title']\n",
    "    X = X.loc[:, X.columns != 'tokenizer_stemmer']\n",
    "    X = X.fillna(0)\n",
    "    print(X.head())\n",
    "    y = df1['title'].astype('category')\n",
    "    y1 = pd.DataFrame(y)\n",
    "    #y1 = pd.Series(LabelEncoder().fit_transform(y))\n",
    "    logreg = LogisticRegression(C=1e90).fit(X, y1)\n",
    "    coefs = list(logreg.coef_)\n",
    "    sums = []\n",
    "    index = 0\n",
    "    while index < len(coefs[0]):\n",
    "        #maxim = coefs[0][index]+ coefs[1][index] +coefs[2][index] +coefs[3][index] +coefs[4][index]\n",
    "        maxim = max(coefs[0][index],coefs[1][index],coefs[2][index],coefs[3][index],coefs[4][index])\n",
    "        sums.append(maxim)\n",
    "        index += 1\n",
    "        logreg_dict[tokenizer_stemmer] = highest_n_numbers(sums, 10)\n",
    "logreg = pd.DataFrame(logreg_dict)\n",
    "\n",
    "logreg_dict_words = {}\n",
    "\n",
    "meep = []\n",
    "\n",
    "for key in logreg_dict.keys():\n",
    "    logreg_dict_words[key] = []\n",
    "    for index in logreg_dict['toktok:porter']:\n",
    "        logreg_dict_words[key].append(df.columns[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the 10 ten terms with the strongest coefficients after TF-IDF transformation of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toktok:porter': ['will',\n",
       "  'without',\n",
       "  'word',\n",
       "  'work',\n",
       "  'www.coxcsrreport.com.',\n",
       "  'year',\n",
       "  'yes',\n",
       "  'yield',\n",
       "  'count',\n",
       "  'with'],\n",
       " 'toktok:stemmer': ['will',\n",
       "  'without',\n",
       "  'word',\n",
       "  'work',\n",
       "  'www.coxcsrreport.com.',\n",
       "  'year',\n",
       "  'yes',\n",
       "  'yield',\n",
       "  'count',\n",
       "  'with'],\n",
       " 'tree:porter': ['will',\n",
       "  'without',\n",
       "  'word',\n",
       "  'work',\n",
       "  'www.coxcsrreport.com.',\n",
       "  'year',\n",
       "  'yes',\n",
       "  'yield',\n",
       "  'count',\n",
       "  'with'],\n",
       " 'tree:stemmer': ['will',\n",
       "  'without',\n",
       "  'word',\n",
       "  'work',\n",
       "  'www.coxcsrreport.com.',\n",
       "  'year',\n",
       "  'yes',\n",
       "  'yield',\n",
       "  'count',\n",
       "  'with']}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logreg_dict_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a marked difference after applying TF-IDF. Common words like we, an, and the semicolon are present when using simple word counts which are not present in TF-IDF. Hence there is no need to specify a set of stop-words, TF-IDF takes care of this by giving low weight to words which occur in all documents. \n",
    "\n",
    "Without TF_IDF: 'divers','scalabl','We','an','better','workstreams.','employ','team.',';','solut'\n",
    "With TF-IDF: 'will','without', 'word','work', 'www.coxcsrreport.com.', 'year', 'yes','yield','count','with'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
